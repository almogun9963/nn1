{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used code from https://github.com/pulunghendroprastyo/ann_chiSquare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\Almog\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Almog\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Almog\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Almog\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Almog\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Almog\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Almog\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Almog\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Almog\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Almog\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Almog\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Almog\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.optimizers import adam \n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset\n",
    "data=pd.read_csv('wpbc.data')\n",
    "data = data.round(4)\n",
    "how_much = data.iloc[: , 1].values\n",
    "temporary = []\n",
    "for num in how_much:\n",
    "    if num == \"R\":\n",
    "        temporary.append(1)\n",
    "    else:\n",
    "        temporary.append(0)\n",
    "data = data.drop(data.columns[1], axis=1)\n",
    "temp = pd.DataFrame(data=temporary, columns=['N'])\n",
    "data = pd.concat([data ,temp], axis=1)\n",
    "\n",
    "# Delete the lines with \"?\" in their features\n",
    "data = data.drop(data.index[5])\n",
    "data = data.drop(data.index[26])\n",
    "data = data.drop(data.index[82])\n",
    "data = data.drop(data.index[192])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(193, 35)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the size of the data\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   N\n",
       "0  0\n",
       "1  0\n",
       "2  0\n",
       "3  1\n",
       "4  1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting up y label.\n",
    "y = data[['N']]\n",
    "y.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>119513</th>\n",
       "      <th>31</th>\n",
       "      <th>18.02</th>\n",
       "      <th>27.6</th>\n",
       "      <th>117.5</th>\n",
       "      <th>1013</th>\n",
       "      <th>0.09489</th>\n",
       "      <th>0.1036</th>\n",
       "      <th>0.1086</th>\n",
       "      <th>0.07055</th>\n",
       "      <th>...</th>\n",
       "      <th>139.7</th>\n",
       "      <th>1436</th>\n",
       "      <th>0.1195</th>\n",
       "      <th>0.1926</th>\n",
       "      <th>0.314</th>\n",
       "      <th>0.117</th>\n",
       "      <th>0.2677</th>\n",
       "      <th>0.08113</th>\n",
       "      <th>5</th>\n",
       "      <th>5.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8423</td>\n",
       "      <td>61</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.1184</td>\n",
       "      <td>0.2776</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.1471</td>\n",
       "      <td>...</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.1189</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>116</td>\n",
       "      <td>21.37</td>\n",
       "      <td>17.44</td>\n",
       "      <td>137.50</td>\n",
       "      <td>1373.0</td>\n",
       "      <td>0.0884</td>\n",
       "      <td>0.1189</td>\n",
       "      <td>0.1255</td>\n",
       "      <td>0.0818</td>\n",
       "      <td>...</td>\n",
       "      <td>159.10</td>\n",
       "      <td>1949.0</td>\n",
       "      <td>0.1188</td>\n",
       "      <td>0.3449</td>\n",
       "      <td>0.3414</td>\n",
       "      <td>0.2032</td>\n",
       "      <td>0.4334</td>\n",
       "      <td>0.0907</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>843483</td>\n",
       "      <td>123</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.1425</td>\n",
       "      <td>0.2839</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.1052</td>\n",
       "      <td>...</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.1730</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>843584</td>\n",
       "      <td>27</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.1003</td>\n",
       "      <td>0.1328</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.1043</td>\n",
       "      <td>...</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.0768</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>843786</td>\n",
       "      <td>77</td>\n",
       "      <td>12.75</td>\n",
       "      <td>15.29</td>\n",
       "      <td>84.60</td>\n",
       "      <td>502.7</td>\n",
       "      <td>0.1189</td>\n",
       "      <td>0.1569</td>\n",
       "      <td>0.1664</td>\n",
       "      <td>0.0767</td>\n",
       "      <td>...</td>\n",
       "      <td>107.30</td>\n",
       "      <td>733.2</td>\n",
       "      <td>0.1706</td>\n",
       "      <td>0.4196</td>\n",
       "      <td>0.5999</td>\n",
       "      <td>0.1709</td>\n",
       "      <td>0.3485</td>\n",
       "      <td>0.1179</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   119513   31  18.02   27.6   117.5    1013  0.09489  0.1036  0.1086  \\\n",
       "0    8423   61  17.99  10.38  122.80  1001.0   0.1184  0.2776  0.3001   \n",
       "1  842517  116  21.37  17.44  137.50  1373.0   0.0884  0.1189  0.1255   \n",
       "2  843483  123  11.42  20.38   77.58   386.1   0.1425  0.2839  0.2414   \n",
       "3  843584   27  20.29  14.34  135.10  1297.0   0.1003  0.1328  0.1980   \n",
       "4  843786   77  12.75  15.29   84.60   502.7   0.1189  0.1569  0.1664   \n",
       "\n",
       "   0.07055  ...   139.7    1436  0.1195  0.1926   0.314   0.117  0.2677  \\\n",
       "0   0.1471  ...  184.60  2019.0  0.1622  0.6656  0.7119  0.2654  0.4601   \n",
       "1   0.0818  ...  159.10  1949.0  0.1188  0.3449  0.3414  0.2032  0.4334   \n",
       "2   0.1052  ...   98.87   567.7  0.2098  0.8663  0.6869  0.2575  0.6638   \n",
       "3   0.1043  ...  152.20  1575.0  0.1374  0.2050  0.4000  0.1625  0.2364   \n",
       "4   0.0767  ...  107.30   733.2  0.1706  0.4196  0.5999  0.1709  0.3485   \n",
       "\n",
       "   0.08113    5  5.1  \n",
       "0   0.1189  3.0    2  \n",
       "1   0.0907  2.5    0  \n",
       "2   0.1730  2.0    0  \n",
       "3   0.0768  3.5    0  \n",
       "4   0.1179  2.5    0  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting up x label, aka the features.\n",
    "X = data.drop(['N'],axis=1)\n",
    "X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spliting\n",
    "x_train, x_test, y_train, y_test = train_test_split(X ,y ,test_size=0.33,random_state=0)\n",
    "y_train = to_categorical(y_train,num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the algorithm backpropagation\n",
    "def backpropagation(x_train,y_train,epochs=150,batch_size=10):\n",
    "    start_time = datetime.now()\n",
    "    inputSize = x_train.shape[1]\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=5, input_dim=inputSize)) \n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(Dense(units=2)) \n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=adam(lr=0.001),metrics=['accuracy'])\n",
    "    \n",
    "    #train NN\n",
    "    mlp=model.fit(x_train, y_train,epochs=epochs, batch_size=batch_size)\n",
    "    end_time = datetime.now()\n",
    "    result_time  =end_time-start_time\n",
    "    ans = model.predict_classes(x_test,batch_size=1)\n",
    "    return result_time, ans\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Almog\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\Almog\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/150\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 2.3327 - accuracy: 0.2248\n",
      "Epoch 2/150\n",
      "129/129 [==============================] - 0s 93us/step - loss: 2.2704 - accuracy: 0.2248\n",
      "Epoch 3/150\n",
      "129/129 [==============================] - 0s 93us/step - loss: 2.2065 - accuracy: 0.2248\n",
      "Epoch 4/150\n",
      "129/129 [==============================] - 0s 116us/step - loss: 2.1432 - accuracy: 0.2248\n",
      "Epoch 5/150\n",
      "129/129 [==============================] - 0s 108us/step - loss: 2.0806 - accuracy: 0.2248\n",
      "Epoch 6/150\n",
      "129/129 [==============================] - 0s 93us/step - loss: 2.0180 - accuracy: 0.2248\n",
      "Epoch 7/150\n",
      "129/129 [==============================] - 0s 93us/step - loss: 1.9529 - accuracy: 0.2248\n",
      "Epoch 8/150\n",
      "129/129 [==============================] - 0s 101us/step - loss: 1.8896 - accuracy: 0.2248\n",
      "Epoch 9/150\n",
      "129/129 [==============================] - 0s 100us/step - loss: 1.8255 - accuracy: 0.2248\n",
      "Epoch 10/150\n",
      "129/129 [==============================] - 0s 85us/step - loss: 1.7642 - accuracy: 0.2248\n",
      "Epoch 11/150\n",
      "129/129 [==============================] - 0s 97us/step - loss: 1.6993 - accuracy: 0.2248\n",
      "Epoch 12/150\n",
      "129/129 [==============================] - 0s 101us/step - loss: 1.6374 - accuracy: 0.2248\n",
      "Epoch 13/150\n",
      "129/129 [==============================] - 0s 101us/step - loss: 1.5744 - accuracy: 0.2248\n",
      "Epoch 14/150\n",
      "129/129 [==============================] - 0s 93us/step - loss: 1.5144 - accuracy: 0.2248\n",
      "Epoch 15/150\n",
      "129/129 [==============================] - 0s 100us/step - loss: 1.4545 - accuracy: 0.2248\n",
      "Epoch 16/150\n",
      "129/129 [==============================] - 0s 116us/step - loss: 1.3958 - accuracy: 0.2248\n",
      "Epoch 17/150\n",
      "129/129 [==============================] - 0s 108us/step - loss: 1.3393 - accuracy: 0.2248\n",
      "Epoch 18/150\n",
      "129/129 [==============================] - 0s 101us/step - loss: 1.2829 - accuracy: 0.2248\n",
      "Epoch 19/150\n",
      "129/129 [==============================] - 0s 124us/step - loss: 1.2293 - accuracy: 0.2248\n",
      "Epoch 20/150\n",
      "129/129 [==============================] - 0s 147us/step - loss: 1.1775 - accuracy: 0.2248\n",
      "Epoch 21/150\n",
      "129/129 [==============================] - 0s 155us/step - loss: 1.1266 - accuracy: 0.2248\n",
      "Epoch 22/150\n",
      "129/129 [==============================] - 0s 116us/step - loss: 1.0805 - accuracy: 0.2248\n",
      "Epoch 23/150\n",
      "129/129 [==============================] - 0s 116us/step - loss: 1.0352 - accuracy: 0.2248\n",
      "Epoch 24/150\n",
      "129/129 [==============================] - 0s 147us/step - loss: 0.9923 - accuracy: 0.2248\n",
      "Epoch 25/150\n",
      "129/129 [==============================] - 0s 155us/step - loss: 0.9530 - accuracy: 0.2248\n",
      "Epoch 26/150\n",
      "129/129 [==============================] - 0s 124us/step - loss: 0.9156 - accuracy: 0.2248\n",
      "Epoch 27/150\n",
      "129/129 [==============================] - 0s 124us/step - loss: 0.8791 - accuracy: 0.2248\n",
      "Epoch 28/150\n",
      "129/129 [==============================] - 0s 116us/step - loss: 0.8474 - accuracy: 0.2248\n",
      "Epoch 29/150\n",
      "129/129 [==============================] - 0s 93us/step - loss: 0.8183 - accuracy: 0.2248\n",
      "Epoch 30/150\n",
      "129/129 [==============================] - 0s 124us/step - loss: 0.7899 - accuracy: 0.2248\n",
      "Epoch 31/150\n",
      "129/129 [==============================] - 0s 124us/step - loss: 0.7649 - accuracy: 0.2248\n",
      "Epoch 32/150\n",
      "129/129 [==============================] - 0s 116us/step - loss: 0.7418 - accuracy: 0.2248\n",
      "Epoch 33/150\n",
      "129/129 [==============================] - 0s 110us/step - loss: 0.7207 - accuracy: 0.2248\n",
      "Epoch 34/150\n",
      "129/129 [==============================] - 0s 124us/step - loss: 0.7023 - accuracy: 0.2481\n",
      "Epoch 35/150\n",
      "129/129 [==============================] - 0s 108us/step - loss: 0.6833 - accuracy: 0.7752\n",
      "Epoch 36/150\n",
      "129/129 [==============================] - 0s 116us/step - loss: 0.6682 - accuracy: 0.7752\n",
      "Epoch 37/150\n",
      "129/129 [==============================] - 0s 124us/step - loss: 0.6549 - accuracy: 0.7752\n",
      "Epoch 38/150\n",
      "129/129 [==============================] - 0s 101us/step - loss: 0.6409 - accuracy: 0.7752\n",
      "Epoch 39/150\n",
      "129/129 [==============================] - 0s 93us/step - loss: 0.6304 - accuracy: 0.7752\n",
      "Epoch 40/150\n",
      "129/129 [==============================] - 0s 93us/step - loss: 0.6193 - accuracy: 0.7752\n",
      "Epoch 41/150\n",
      "129/129 [==============================] - 0s 124us/step - loss: 0.6110 - accuracy: 0.7752\n",
      "Epoch 42/150\n",
      "129/129 [==============================] - 0s 124us/step - loss: 0.6013 - accuracy: 0.7752\n",
      "Epoch 43/150\n",
      "129/129 [==============================] - 0s 124us/step - loss: 0.5946 - accuracy: 0.7752\n",
      "Epoch 44/150\n",
      "129/129 [==============================] - 0s 116us/step - loss: 0.5873 - accuracy: 0.7752\n",
      "Epoch 45/150\n",
      "129/129 [==============================] - 0s 116us/step - loss: 0.5819 - accuracy: 0.7752\n",
      "Epoch 46/150\n",
      "129/129 [==============================] - 0s 124us/step - loss: 0.5762 - accuracy: 0.7752\n",
      "Epoch 47/150\n",
      "129/129 [==============================] - 0s 116us/step - loss: 0.5713 - accuracy: 0.7752\n",
      "Epoch 48/150\n",
      "129/129 [==============================] - 0s 116us/step - loss: 0.5672 - accuracy: 0.7752\n",
      "Epoch 49/150\n",
      "129/129 [==============================] - 0s 155us/step - loss: 0.5636 - accuracy: 0.7752\n",
      "Epoch 50/150\n",
      "129/129 [==============================] - 0s 131us/step - loss: 0.5599 - accuracy: 0.7752\n",
      "Epoch 51/150\n",
      "129/129 [==============================] - 0s 116us/step - loss: 0.5570 - accuracy: 0.7752\n",
      "Epoch 52/150\n",
      "129/129 [==============================] - 0s 124us/step - loss: 0.5541 - accuracy: 0.7752\n",
      "Epoch 53/150\n",
      "129/129 [==============================] - 0s 124us/step - loss: 0.5519 - accuracy: 0.7752\n",
      "Epoch 54/150\n",
      "129/129 [==============================] - 0s 108us/step - loss: 0.5498 - accuracy: 0.7752\n",
      "Epoch 55/150\n",
      "129/129 [==============================] - 0s 101us/step - loss: 0.5478 - accuracy: 0.7752\n",
      "Epoch 56/150\n",
      "129/129 [==============================] - 0s 108us/step - loss: 0.5460 - accuracy: 0.7752\n",
      "Epoch 57/150\n",
      "129/129 [==============================] - 0s 186us/step - loss: 0.5445 - accuracy: 0.7752\n",
      "Epoch 58/150\n",
      "129/129 [==============================] - 0s 116us/step - loss: 0.5436 - accuracy: 0.7752\n",
      "Epoch 59/150\n",
      "129/129 [==============================] - 0s 131us/step - loss: 0.5420 - accuracy: 0.7752\n",
      "Epoch 60/150\n",
      "129/129 [==============================] - 0s 131us/step - loss: 0.5409 - accuracy: 0.7752\n",
      "Epoch 61/150\n",
      "129/129 [==============================] - 0s 100us/step - loss: 0.5401 - accuracy: 0.7752\n",
      "Epoch 62/150\n",
      "129/129 [==============================] - 0s 116us/step - loss: 0.5393 - accuracy: 0.7752\n",
      "Epoch 63/150\n",
      "129/129 [==============================] - 0s 104us/step - loss: 0.5385 - accuracy: 0.7752\n",
      "Epoch 64/150\n",
      "129/129 [==============================] - 0s 108us/step - loss: 0.5378 - accuracy: 0.7752\n",
      "Epoch 65/150\n",
      "129/129 [==============================] - 0s 101us/step - loss: 0.5375 - accuracy: 0.7752\n",
      "Epoch 66/150\n",
      "129/129 [==============================] - 0s 100us/step - loss: 0.5367 - accuracy: 0.7752\n",
      "Epoch 67/150\n",
      "129/129 [==============================] - 0s 131us/step - loss: 0.5363 - accuracy: 0.7752\n",
      "Epoch 68/150\n",
      "129/129 [==============================] - 0s 116us/step - loss: 0.5359 - accuracy: 0.7752\n",
      "Epoch 69/150\n",
      "129/129 [==============================] - 0s 93us/step - loss: 0.5355 - accuracy: 0.7752\n",
      "Epoch 70/150\n",
      "129/129 [==============================] - 0s 101us/step - loss: 0.5353 - accuracy: 0.7752\n",
      "Epoch 71/150\n",
      "129/129 [==============================] - 0s 108us/step - loss: 0.5349 - accuracy: 0.7752\n",
      "Epoch 72/150\n",
      "129/129 [==============================] - 0s 108us/step - loss: 0.5348 - accuracy: 0.7752\n",
      "Epoch 73/150\n",
      "129/129 [==============================] - 0s 93us/step - loss: 0.5345 - accuracy: 0.7752\n",
      "Epoch 74/150\n",
      "129/129 [==============================] - 0s 93us/step - loss: 0.5343 - accuracy: 0.7752\n",
      "Epoch 75/150\n",
      "129/129 [==============================] - 0s 108us/step - loss: 0.5341 - accuracy: 0.7752\n",
      "Epoch 76/150\n",
      "129/129 [==============================] - 0s 101us/step - loss: 0.5340 - accuracy: 0.7752\n",
      "Epoch 77/150\n",
      "129/129 [==============================] - 0s 101us/step - loss: 0.5340 - accuracy: 0.7752\n",
      "Epoch 78/150\n",
      "129/129 [==============================] - 0s 101us/step - loss: 0.5339 - accuracy: 0.7752\n",
      "Epoch 79/150\n",
      "129/129 [==============================] - 0s 101us/step - loss: 0.5338 - accuracy: 0.7752\n",
      "Epoch 80/150\n",
      "129/129 [==============================] - 0s 93us/step - loss: 0.5335 - accuracy: 0.7752\n",
      "Epoch 81/150\n",
      "129/129 [==============================] - ETA: 0s - loss: 0.3886 - accuracy: 0.90 - 0s 93us/step - loss: 0.5335 - accuracy: 0.7752\n",
      "Epoch 82/150\n",
      "129/129 [==============================] - 0s 77us/step - loss: 0.5334 - accuracy: 0.7752\n",
      "Epoch 83/150\n",
      "129/129 [==============================] - 0s 93us/step - loss: 0.5334 - accuracy: 0.7752\n",
      "Epoch 84/150\n",
      "129/129 [==============================] - 0s 93us/step - loss: 0.5333 - accuracy: 0.7752\n",
      "Epoch 85/150\n",
      "129/129 [==============================] - 0s 93us/step - loss: 0.5334 - accuracy: 0.7752\n",
      "Epoch 86/150\n",
      "129/129 [==============================] - 0s 93us/step - loss: 0.5332 - accuracy: 0.7752\n",
      "Epoch 87/150\n",
      "129/129 [==============================] - 0s 116us/step - loss: 0.5332 - accuracy: 0.7752\n",
      "Epoch 88/150\n",
      "129/129 [==============================] - 0s 108us/step - loss: 0.5333 - accuracy: 0.7752\n",
      "Epoch 89/150\n",
      "129/129 [==============================] - 0s 108us/step - loss: 0.5331 - accuracy: 0.7752\n",
      "Epoch 90/150\n",
      "129/129 [==============================] - 0s 124us/step - loss: 0.5331 - accuracy: 0.7752\n",
      "Epoch 91/150\n",
      "129/129 [==============================] - 0s 101us/step - loss: 0.5331 - accuracy: 0.7752\n",
      "Epoch 92/150\n",
      "129/129 [==============================] - 0s 101us/step - loss: 0.5331 - accuracy: 0.7752\n",
      "Epoch 93/150\n",
      "129/129 [==============================] - 0s 116us/step - loss: 0.5331 - accuracy: 0.7752\n",
      "Epoch 94/150\n",
      "129/129 [==============================] - 0s 108us/step - loss: 0.5330 - accuracy: 0.7752\n",
      "Epoch 95/150\n",
      "129/129 [==============================] - 0s 131us/step - loss: 0.5331 - accuracy: 0.7752\n",
      "Epoch 96/150\n",
      "129/129 [==============================] - 0s 93us/step - loss: 0.5330 - accuracy: 0.7752\n",
      "Epoch 97/150\n",
      "129/129 [==============================] - 0s 93us/step - loss: 0.5330 - accuracy: 0.7752\n",
      "Epoch 98/150\n",
      "129/129 [==============================] - 0s 93us/step - loss: 0.5330 - accuracy: 0.7752\n",
      "Epoch 99/150\n",
      "129/129 [==============================] - 0s 93us/step - loss: 0.5330 - accuracy: 0.7752\n",
      "Epoch 100/150\n",
      "129/129 [==============================] - 0s 93us/step - loss: 0.5331 - accuracy: 0.7752\n",
      "Epoch 101/150\n",
      "129/129 [==============================] - 0s 101us/step - loss: 0.5331 - accuracy: 0.7752\n",
      "Epoch 102/150\n",
      "129/129 [==============================] - 0s 93us/step - loss: 0.5331 - accuracy: 0.7752\n",
      "Epoch 103/150\n",
      "129/129 [==============================] - 0s 108us/step - loss: 0.5331 - accuracy: 0.7752\n",
      "Epoch 104/150\n",
      "129/129 [==============================] - 0s 108us/step - loss: 0.5330 - accuracy: 0.7752\n",
      "Epoch 105/150\n",
      "129/129 [==============================] - 0s 100us/step - loss: 0.5330 - accuracy: 0.7752\n",
      "Epoch 106/150\n",
      "129/129 [==============================] - 0s 85us/step - loss: 0.5330 - accuracy: 0.7752\n",
      "Epoch 107/150\n",
      "129/129 [==============================] - 0s 93us/step - loss: 0.5333 - accuracy: 0.7752\n",
      "Epoch 108/150\n",
      "129/129 [==============================] - 0s 93us/step - loss: 0.5331 - accuracy: 0.7752\n",
      "Epoch 109/150\n",
      "129/129 [==============================] - 0s 77us/step - loss: 0.5330 - accuracy: 0.7752\n",
      "Epoch 110/150\n",
      "129/129 [==============================] - 0s 93us/step - loss: 0.5331 - accuracy: 0.7752\n",
      "Epoch 111/150\n",
      "129/129 [==============================] - 0s 116us/step - loss: 0.5330 - accuracy: 0.7752\n",
      "Epoch 112/150\n",
      "129/129 [==============================] - 0s 116us/step - loss: 0.5330 - accuracy: 0.7752\n",
      "Epoch 113/150\n",
      "129/129 [==============================] - 0s 93us/step - loss: 0.5330 - accuracy: 0.7752\n",
      "Epoch 114/150\n",
      "129/129 [==============================] - 0s 101us/step - loss: 0.5330 - accuracy: 0.7752\n",
      "Epoch 115/150\n",
      "129/129 [==============================] - 0s 101us/step - loss: 0.5331 - accuracy: 0.7752\n",
      "Epoch 116/150\n",
      "129/129 [==============================] - 0s 108us/step - loss: 0.5330 - accuracy: 0.7752\n",
      "Epoch 117/150\n",
      "129/129 [==============================] - 0s 101us/step - loss: 0.5330 - accuracy: 0.7752\n",
      "Epoch 118/150\n",
      "129/129 [==============================] - 0s 101us/step - loss: 0.5331 - accuracy: 0.7752\n",
      "Epoch 119/150\n",
      "129/129 [==============================] - 0s 85us/step - loss: 0.5331 - accuracy: 0.7752\n",
      "Epoch 120/150\n",
      "129/129 [==============================] - 0s 93us/step - loss: 0.5330 - accuracy: 0.7752\n",
      "Epoch 121/150\n",
      "129/129 [==============================] - ETA: 0s - loss: 0.3785 - accuracy: 0.90 - 0s 93us/step - loss: 0.5329 - accuracy: 0.7752\n",
      "Epoch 122/150\n",
      "129/129 [==============================] - 0s 100us/step - loss: 0.5330 - accuracy: 0.7752\n",
      "Epoch 123/150\n",
      "129/129 [==============================] - 0s 93us/step - loss: 0.5331 - accuracy: 0.7752\n",
      "Epoch 124/150\n",
      "129/129 [==============================] - 0s 101us/step - loss: 0.5334 - accuracy: 0.7752\n",
      "Epoch 125/150\n",
      "129/129 [==============================] - 0s 147us/step - loss: 0.5330 - accuracy: 0.7752\n",
      "Epoch 126/150\n",
      "129/129 [==============================] - 0s 139us/step - loss: 0.5330 - accuracy: 0.7752\n",
      "Epoch 127/150\n",
      "129/129 [==============================] - 0s 131us/step - loss: 0.5330 - accuracy: 0.7752\n",
      "Epoch 128/150\n",
      "129/129 [==============================] - 0s 124us/step - loss: 0.5332 - accuracy: 0.7752\n",
      "Epoch 129/150\n",
      "129/129 [==============================] - 0s 116us/step - loss: 0.5330 - accuracy: 0.7752\n",
      "Epoch 130/150\n",
      "129/129 [==============================] - 0s 93us/step - loss: 0.5331 - accuracy: 0.7752\n",
      "Epoch 131/150\n",
      "129/129 [==============================] - 0s 93us/step - loss: 0.5330 - accuracy: 0.7752\n",
      "Epoch 132/150\n",
      "129/129 [==============================] - 0s 108us/step - loss: 0.5329 - accuracy: 0.7752\n",
      "Epoch 133/150\n",
      "129/129 [==============================] - 0s 101us/step - loss: 0.5330 - accuracy: 0.7752\n",
      "Epoch 134/150\n",
      "129/129 [==============================] - 0s 101us/step - loss: 0.5330 - accuracy: 0.7752\n",
      "Epoch 135/150\n",
      "129/129 [==============================] - 0s 93us/step - loss: 0.5332 - accuracy: 0.7752\n",
      "Epoch 136/150\n",
      "129/129 [==============================] - 0s 93us/step - loss: 0.5330 - accuracy: 0.7752\n",
      "Epoch 137/150\n",
      "129/129 [==============================] - 0s 93us/step - loss: 0.5330 - accuracy: 0.7752\n",
      "Epoch 138/150\n",
      "129/129 [==============================] - ETA: 0s - loss: 0.5022 - accuracy: 0.80 - 0s 85us/step - loss: 0.5330 - accuracy: 0.7752\n",
      "Epoch 139/150\n",
      "129/129 [==============================] - 0s 100us/step - loss: 0.5330 - accuracy: 0.7752\n",
      "Epoch 140/150\n",
      "129/129 [==============================] - 0s 97us/step - loss: 0.5331 - accuracy: 0.7752\n",
      "Epoch 141/150\n",
      "129/129 [==============================] - 0s 108us/step - loss: 0.5332 - accuracy: 0.7752\n",
      "Epoch 142/150\n",
      "129/129 [==============================] - 0s 101us/step - loss: 0.5329 - accuracy: 0.7752\n",
      "Epoch 143/150\n",
      "129/129 [==============================] - 0s 93us/step - loss: 0.5330 - accuracy: 0.7752\n",
      "Epoch 144/150\n",
      "129/129 [==============================] - 0s 93us/step - loss: 0.5330 - accuracy: 0.7752\n",
      "Epoch 145/150\n",
      "129/129 [==============================] - 0s 85us/step - loss: 0.5330 - accuracy: 0.7752\n",
      "Epoch 146/150\n",
      "129/129 [==============================] - 0s 101us/step - loss: 0.5332 - accuracy: 0.7752\n",
      "Epoch 147/150\n",
      "129/129 [==============================] - 0s 85us/step - loss: 0.5330 - accuracy: 0.7752\n",
      "Epoch 148/150\n",
      "129/129 [==============================] - 0s 85us/step - loss: 0.5331 - accuracy: 0.7752\n",
      "Epoch 149/150\n",
      "129/129 [==============================] - 0s 93us/step - loss: 0.5330 - accuracy: 0.7752\n",
      "Epoch 150/150\n",
      "129/129 [==============================] - 0s 108us/step - loss: 0.5331 - accuracy: 0.7752\n"
     ]
    }
   ],
   "source": [
    "time, ans = backpropagation(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 0:00:03.182684\n"
     ]
    }
   ],
   "source": [
    "print(\"Duration:\",time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRUE NEGATIVE (TN): 47\n",
      "FALSE NEGATIVE (FN): 0\n",
      "TRUE POSITIVE (TP): 0\n",
      "FALSE POSITIVE (FP): 17\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.73      0.85        64\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.73        64\n",
      "   macro avg       0.50      0.37      0.42        64\n",
      "weighted avg       1.00      0.73      0.85        64\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Almog\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#getting the TRUE NEGATIVE,FALSE NEGATIVE,FALSE POSITIVE\n",
    "CM = confusion_matrix(ans,y_test)\n",
    "TN = CM[0][0]\n",
    "FN = CM[1][0]\n",
    "TP = CM[1][1]\n",
    "FP = CM[0][1]\n",
    "\n",
    "print (\"TRUE NEGATIVE (TN):\",TN)\n",
    "print (\"FALSE NEGATIVE (FN):\",FN)\n",
    "print (\"TRUE POSITIVE (TP):\",TP)\n",
    "print (\"FALSE POSITIVE (FP):\",FP)\n",
    "print(classification_report(ans,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6589743589743591, 0.6589743589743591, 0.6512820512820513, 0.7057692307692307, 0.6903846153846154, 0.7057692307692307, 0.6897435897435897, 0.6974358974358974, 0.6820512820512821, 0.7051282051282052, 0.7051282051282052, 0.7128205128205128, 0.7128205128205128, 0.7128205128205128, 0.7134615384615384, 0.7211538461538461, 0.7288461538461539, 0.7211538461538461, 0.7288461538461539, 0.7211538461538461, 0.7211538461538461, 0.7211538461538461, 0.7288461538461539, 0.7288461538461539, 0.7288461538461539, 0.7288461538461539, 0.7288461538461539, 0.7288461538461539, 0.7288461538461539, 0.7288461538461539, 0.7288461538461539, 0.7288461538461539, 0.7288461538461539, 0.7365384615384616, 0.7365384615384616, 0.7365384615384616, 0.7365384615384616, 0.7365384615384616, 0.7365384615384616, 0.7365384615384616]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# create the model\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "# Enter the data training to the mode\n",
    "knn.fit(x_train, y_train)\n",
    "y_pred = knn.predict(x_test)\n",
    "# print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "# using cross validation to get the best percentage of success\n",
    "\n",
    "k_range = list(range(1, 41))\n",
    "k_scores = []\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, weights='distance', p=1)\n",
    "    scores = cross_val_score(knn, x_train, y_train, cv=10, scoring='accuracy')\n",
    "    k_scores.append(scores.mean())\n",
    "print(k_scores)\n",
    "# graph of the value of k (x) and the cross validation accuracy\n",
    "plt.plot(k_range, k_scores)\n",
    "# names of x and y\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Cross validation Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
